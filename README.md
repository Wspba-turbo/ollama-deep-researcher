# Ollama Deep Researcher

Ollama Deep Researcher is a fully local web research assistant that uses any LLM hosted by [Ollama](https://ollama.com/search). Give it a topic and it will generate a web search query, gather web search results (via [Tavily](https://www.tavily.com/) by default), summarize the results of web search, reflect on the summary to examine knowledge gaps, generate a new search query to address the gaps, search, and improve the summary for a user-defined number of cycles. It will provide the user a final markdown summary with all sources used. 

![research-rabbit](https://github.com/user-attachments/assets/4308ee9c-abf3-4abb-9d1e-83e7c2c3f187)

Short summary:
<video src="https://github.com/user-attachments/assets/02084902-f067-4658-9683-ff312cab7944" controls></video>

## ğŸ“º Video Tutorials

See it in action or build it yourself? Check out these helpful video tutorials:
- [Overview of Ollama Deep Researcher with R1](https://www.youtube.com/watch?v=sGUjmyfof4Q) - Load and test [DeepSeek R1](https://api-docs.deepseek.com/news/news250120) [distilled models](https://ollama.com/library/deepseek-r1).
- [Building Ollama Deep Researcher from Scratch](https://www.youtube.com/watch?v=XGuTzHoqlj8) - Overview of how this is built.

## ğŸš€ Quickstart

### Mac 

1. Download the Ollama app for Mac [here](https://ollama.com/download).

2. Pull a local LLM from [Ollama](https://ollama.com/search). As an [example](https://ollama.com/library/deepseek-r1:8b): 
```bash
ollama pull deepseek-r1:8b
```

3. For free web search (up to 1000 requests), [sign up for Tavily](https://tavily.com/). 

4. Set the `TAVILY_API_KEY` environment variable and restart your terminal to ensure it is set:

```bash
export TAVILY_API_KEY=<your_tavily_api_key>
```

5. (Recommended) Create a virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate
```

6. Clone the repository and launch the assistant with the LangGraph server:

```bash
# Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone the repository and start the LangGraph server 
git clone https://github.com/langchain-ai/ollama-deep-researcher.git
cd ollama-deep-researcher
uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.11 langgraph dev
```

### Windows 

1. Download the Ollama app for Windows [here](https://ollama.com/download).

2. Pull a local LLM from [Ollama](https://ollama.com/search). As an [example](https://ollama.com/library/deepseek-r1:8b): 
```powershell
ollama pull deepseek-r1:8b
```

3. For free web search (up to 1000 requests), [sign up for Tavily](https://tavily.com/). 

4. Set the `TAVILY_API_KEY` environment variable in Windows (via System Properties or PowerShell). Crucially, restart your terminal/IDE (or sometimes even your computer) after setting it for the change to take effect.

5. (Recommended) Create a virtual environment: Install `Python 3.11` (and add to PATH during installation). Restart your terminal to ensure Python is available, then create and activate a virtual environment:

```powershell
python -m venv .venv
.venv\Scripts\Activate.ps1
```

6. Clone the repository and launch the assistant with the LangGraph server:

```powershell
# Clone the repository 
git clone https://github.com/langchain-ai/ollama-deep-researcher.git
cd ollama-deep-researcher

# Install dependencies 
pip install -e .
pip install langgraph-cli[inmem]

# Start the LangGraph server
langgraph dev
```

## ä½¿ç”¨æŠ€æœ¯å…¨æ™¯å›¾ç”ŸæˆåŠŸèƒ½

ä½ å¯ä»¥ä½¿ç”¨ `generate_tech_landscape.py` è„šæœ¬æ¥ç”ŸæˆæŠ€æœ¯å…¨æ™¯å›¾ã€‚è¯¥è„šæœ¬ä¼šå¯¹æŒ‡å®šçš„æŠ€æœ¯è¿›è¡Œæ·±å…¥åˆ†æï¼Œç”Ÿæˆç›¸å…³æŠ€æœ¯çš„æ ‘çŠ¶å›¾ï¼Œå¹¶æä¾›å¯è§†åŒ–ç»“æœã€‚

### åŸºæœ¬ç”¨æ³•

```bash
python generate_tech_landscape.py <æŠ€æœ¯åç§°> [é€‰é¡¹]
```

### å‚æ•°è¯´æ˜

- `æŠ€æœ¯åç§°`: è¦åˆ†æçš„æ ¹æŠ€æœ¯åç§°
- `--output`, `-o`: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼štech_landscape.jsonï¼‰
- `--depth`, `-d`: æŠ€æœ¯æ ‘çš„æœ€å¤§æ·±åº¦ï¼ˆé»˜è®¤ï¼š2ï¼‰
- `--max-related`, `-m`: æ¯ä¸ªèŠ‚ç‚¹çš„ç›¸å…³æŠ€æœ¯æ•°é‡ä¸Šé™ï¼ˆé»˜è®¤ï¼š5ï¼‰
- `--language`, `-l`: è¾“å‡ºè¯­è¨€ï¼ˆé»˜è®¤ï¼šEnglishï¼‰
- `--model`: ä½¿ç”¨çš„ Ollama æ¨¡å‹ï¼ˆé»˜è®¤ï¼šmistralï¼‰
- `--iterations`, `-i`: æ¯ä¸ªæŠ€æœ¯çš„ç ”ç©¶è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤ï¼š2ï¼‰

### ç¤ºä¾‹

1. åŸºæœ¬ä½¿ç”¨ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰ï¼š
```bash
python generate_tech_landscape.py "AI Agent"
```

2. æŒ‡å®šå‚æ•°ï¼š
```bash
python generate_tech_landscape.py "AI Agent" -d 1 -m 3 -l "English" --model mistral --iterations 2
```

### è¾“å‡ºæ–‡ä»¶

è„šæœ¬ä¼šç”Ÿæˆä¸¤ä¸ªè¾“å‡ºæ–‡ä»¶ï¼š
1. JSON æ–‡ä»¶ï¼ˆé»˜è®¤ï¼štech_landscape.jsonï¼‰ï¼šåŒ…å«å®Œæ•´çš„æŠ€æœ¯åˆ†æç»“æœï¼ŒåŒ…æ‹¬ï¼š
   - æŠ€æœ¯åç§°
   - æŠ€æœ¯æè¿°
   - æ·±åº¦çº§åˆ«
   - å†å²æ€»ç»“
   - ç›¸å…³æŠ€æœ¯åˆ—è¡¨

2. PNG å›¾ç‰‡ï¼ˆä¾‹å¦‚ï¼šai_agent_landscape.pngï¼‰ï¼šå¯è§†åŒ–çš„æŠ€æœ¯å…³ç³»å›¾
   - ä»¥èŠ‚ç‚¹å’Œè¾¹å±•ç¤ºæŠ€æœ¯ä¹‹é—´çš„å…³ç³»
   - ä½¿ç”¨ä¸åŒçš„é¢œè‰²å’Œå¤§å°æ¥è¡¨ç¤ºæŠ€æœ¯çš„å±‚çº§
   - æ¸…æ™°å±•ç¤ºæŠ€æœ¯ä¹‹é—´çš„å…³è”æ€§

## How it works

[Original content continues below...]

[Previous content remains unchanged]
